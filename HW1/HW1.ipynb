{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1Xhk6gNzM6ub5vNj4FxlE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisP-Bakein6741/Gen_AI2026/blob/main/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install & import the needed libraries\n",
        "\n",
        "!pip install -q transformers torch\n",
        "\n",
        "!pip install triton torchao\n",
        "\n"
      ],
      "metadata": {
        "id": "OCZtX6byUBu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337fb0d6-9b69-4bb7-9523-95f283b6823d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (0.10.0)\n",
            "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import os\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\" # Disables progress bar widgets error caused by GPT\n"
      ],
      "metadata": {
        "id": "VH5GmrTgUI8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer & model\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "M2RFvfcyUbX0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804,
          "referenced_widgets": [
            "9cfe276fa55f4f399814a9062286b050",
            "aee889c2ef9a41fb9737753928f0a538",
            "4b6ab9aa13ab4c5c8cdd9146c2d14a12",
            "a9505fb2a88747698e70fde192fc8c39",
            "a8141673d9bd489881f8e81361178681",
            "96921430210e45488bf29977579f66e9",
            "9506305e86ee48a9aa25d94bacd3fac7",
            "e10fc2e083e541d6988b001d2e0cfe25",
            "ee38e518e46e4e3ab61bb5b1c57f9590",
            "f5fb78426ff84128aa661a6b8771da74",
            "66fad6fefcce4cb1a928be6ec3ffb11a",
            "ae85f97bf71743d49581a8c76aaeb2e1",
            "2c6df6b7139a48d1b9dfd98b97589e62",
            "2254c606547c412ba6785542ee52da0f",
            "75c25e98383a42a9898d72df132702b9",
            "d5e46fdd2bdc491ba68ec684ec58d498",
            "c9f89a6ee3534457bc09de44762d9d1e",
            "e3f48715db184a00ae9bf22081d15ef1",
            "2102e09f7d884399b28a8c3f9a109421",
            "26b02b138956411e9bceb9cd3c9936de",
            "7b9c6ef1b7664703a4f4d4e379ccfbcc",
            "ca8fb6f886864b869445fb583e8f026a",
            "14fc710aa74f4500bc9a1f5f4a7d733d",
            "df16583e98d04be6b073015208c53be9",
            "a157d5ed61e644ceaf379fb58d25da4e",
            "ea0c92b1ffe34ee1af6dd4d367cf476d",
            "4847acdf57114e29a1d70255d587aa99",
            "9026f49fa2f84e339a9d2e360a91c5d2",
            "fe697637dbdb4114aa762bb3b15d3b3d",
            "2210410dbe9847178920e027768ef6a4",
            "847ec4ff6dd34441988f3fc2a5c5496f",
            "ebb4aa14653b4a90a73bda279bcf8c7a",
            "3d17aa6880b14b74aca01a8479476f72",
            "f1dbbd065c244b2a869ba9a975f872f8",
            "9fc13f65788d47a4a9abbb9bd5a2a886",
            "a3c01c7b42e84b1cb68727af154031c2",
            "c855c0dd8386437f9e899c11154f1e23",
            "4b78a760efe9421fb457afa92dc37c53",
            "a529b04ad6804f69a371796c35dee19a",
            "a30cf76e065145f99c29b6cf1c91ae78",
            "72a1eb29838e4e6aab8f6064a2687dbb",
            "81dc62f8871b484490ced35fbe7ee221",
            "1fe93a4442dc4d6dafd476696d93fc36",
            "4262750ccb54450c9286e6fbce4823eb",
            "b2af61786cf445fca73c20517e553c9a",
            "acc9820c029d49c2a3e9335c7b73179f",
            "a42df531e82e4f22aa45cbd006ad55a5",
            "eb972e6a416540878aa5d8a0d44aea46",
            "233020c7562a47b197b4d47c98d8d353",
            "cc2334f72c9941c78016836940cbaf62",
            "dae882b128114350b1d2180f37d4da38",
            "1f39ec7eb7314e589bb49a98ab7a75b5",
            "a163fd2eef2d419ab4e0531921843e6e",
            "9596d40ea1704ef2a90807510d0f18b1",
            "3b4d098bf1e14630b27ccd01e85ff00a",
            "e68cbbcb194541b78d48d53975d9f7c6",
            "668c2f03396642d690fcd4b4b4773c9a",
            "6739c5ddd8074728a8b7e96902a64c0c",
            "36124b5f45d645088c56ceeb30f06793",
            "8f2e5d3f81d64393b4caf0108fff43b8",
            "7f3bb4822e274733a9dae45689958adc",
            "448b08316df84e1a9779ea6ff5bfbddf",
            "8589417d848f47278fa5f1e24e36f4d8",
            "1221b1ae0d0a4701923257de4947b9ae",
            "a0cec8d592284104b33aea73e2d6b502",
            "251e8dda50d14beea5f9cd9e2b2650e0",
            "cf00d6a7c5994e778bebd947bac3a869",
            "cb1fa04b797340d9a02f17b885499037",
            "d9a29926dede46a7b2f348d4d081d2a5",
            "2225b5fc016f45af94e8ca374c3ea38c",
            "4f7a17c8a4444766a91fec5a902e10fe",
            "ae51c475c8c8463b941b87260b406194",
            "09856fd1c8814069ad46936dd43a123a",
            "5a1a6210e7b943e0addf2c9849720341",
            "6c8815b6ecab427b869afad81711a750",
            "2eb8cb61eb424327945fd5ab56282868",
            "de7c7c8122ec4617b97946e52934ee70"
          ]
        },
        "outputId": "ef2900c5-4263-446e-ccd1-c92ddf0b745c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cfe276fa55f4f399814a9062286b050"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae85f97bf71743d49581a8c76aaeb2e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14fc710aa74f4500bc9a1f5f4a7d733d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1dbbd065c244b2a869ba9a975f872f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2af61786cf445fca73c20517e553c9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e68cbbcb194541b78d48d53975d9f7c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf00d6a7c5994e778bebd947bac3a869"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your own input text\n",
        "\n",
        "text = input(\"Enter a sentence: \")\n"
      ],
      "metadata": {
        "id": "meoOLg5yUmQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7dbfe1d-b74c-412e-d4d6-c4b25fa27e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: the cat sat on the mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The tokenization step typically creates subword tokens, and not necessarily whole words\n",
        "\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Token IDs:\", tokens.tolist()[0])\n",
        "print(\"Tokens:\")\n",
        "for tid in tokens[0]:\n",
        "    print(f\"{tid.item():>6} → '{tokenizer.decode(tid)}'\")\n"
      ],
      "metadata": {
        "id": "HNzXV2M7U6CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e897f5-158c-48b6-edcf-c69507280ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [1169, 3797, 3332, 319, 262, 2603]\n",
            "Tokens:\n",
            "  1169 → 'the'\n",
            "  3797 → ' cat'\n",
            "  3332 → ' sat'\n",
            "   319 → ' on'\n",
            "   262 → ' the'\n",
            "  2603 → ' mat'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The embeddings\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Token embeddings\n",
        "    token_embeds = model.transformer.wte(tokens)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(tokens.size(1)).unsqueeze(0)\n",
        "    pos_embeds = model.transformer.wpe(positions)\n",
        "\n",
        "    embeddings = token_embeds + pos_embeds\n",
        "\n",
        "# (batch_size, sequence_length, embedding_dim)\n",
        "print(\"Embedding shape:\", embeddings.shape)\n"
      ],
      "metadata": {
        "id": "hWWDeF8uVT9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2d127a-7977-4bf8-b737-dd233493083c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([1, 6, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The transformer forward pass ensures that each token now contains contextual information from previous tokens.\n",
        "# This is the most important step conceptually, because this is where the model goes from isolated words to understanding a sentence.\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Send the embedding vectors through all transformer layers (for GPT-2, it is 12 layers)\n",
        "    outputs = model.transformer(inputs_embeds=embeddings)\n",
        "\n",
        "    # Each layer, applies the self-attention mechanism and goes through a feed-forward NN\n",
        "    hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(\"Hidden state shape:\", hidden_states.shape)\n"
      ],
      "metadata": {
        "id": "MTYhtAE8VqEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba669dfc-43df-4c26-dd9f-bbeb56169d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden state shape: torch.Size([1, 6, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logits for the next token. This gives one score per vocabulary token (~50k tokens)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden = hidden_states[:, -1, :]\n",
        "    logits = model.lm_head(last_hidden)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)\n"
      ],
      "metadata": {
        "id": "IxcEuLd1V4Vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490014e9-c763-43a1-bf14-9df5c06444af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([1, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logits for the next token. This gives one score per vocabulary token (~50k tokens)\n",
        "with torch.no_grad():\n",
        "    last_hidden = hidden_states[:, -1, :]\n",
        "    logits = model.lm_head(last_hidden)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "\n",
        "# Softmax → probabilities: this is the actual probability distribution the model uses\n",
        "\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "top_probs, top_ids = torch.topk(probs, k=10)\n",
        "\n",
        "print(\"Top 10 next-token probabilities:\")\n",
        "for p, tid in zip(top_probs[0], top_ids[0]):\n",
        "    token = tokenizer.decode(tid)\n",
        "    print(f\"{token!r:>12} : {p.item():.4f}\")"
      ],
      "metadata": {
        "id": "OvRMw5ROWGvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec63c67-f52b-4003-e420-bc1e31cad430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([1, 50257])\n",
            "Top 10 next-token probabilities:\n",
            "         ',' : 0.3282\n",
            "         '.' : 0.1971\n",
            "      ' and' : 0.1621\n",
            "       ' in' : 0.0600\n",
            "     ' with' : 0.0473\n",
            "      ' for' : 0.0293\n",
            "       ' as' : 0.0139\n",
            "       ' on' : 0.0116\n",
            "    ' while' : 0.0108\n",
            "       ' to' : 0.0096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling (temperature + top-k)\n",
        "\n",
        "#    temperature = 0.2 (Set a low temperature value to generate predictable responses)\n",
        "#    temperature = 1.5 (Set a high temperature value to generate more random and creative responses)\n",
        "#    top_k = None (full distribution)\n",
        "\n",
        "def sample_next_token(logits, temperature=1.2, top_k=50):\n",
        "    logits = logits / temperature\n",
        "\n",
        "    if top_k is not None:\n",
        "        values, indices = torch.topk(logits, top_k)\n",
        "        probs = F.softmax(values, dim=-1)\n",
        "        choice = torch.multinomial(probs, 1)\n",
        "        return indices[0, choice]\n",
        "    else:\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return torch.multinomial(probs, 1)\n",
        "\n",
        "next_token_id = sample_next_token(logits, temperature=0.8, top_k=40)\n",
        "print(\"Sampled token:\", tokenizer.decode(next_token_id[0]))\n"
      ],
      "metadata": {
        "id": "XXilv6iIWS4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d5ab2c-1567-4c19-ade8-19ef1c821cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled token:  in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full loop (generate multiple tokens)\n",
        "\n",
        "def generate_step_by_step(prompt, steps=10):\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tokens)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            next_token = sample_next_token(logits, temperature=0.1, top_k=40)\n",
        "\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "        print(tokenizer.decode(tokens[0]))\n",
        "\n",
        "generate_step_by_step(text, steps=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb1051b-8c79-490c-86ca-1f64294e300c",
        "id": "2o5VEu-LJM4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cat sat on the mat,\n",
            "the cat sat on the mat, and\n",
            "the cat sat on the mat, and the\n",
            "the cat sat on the mat, and the cat\n",
            "the cat sat on the mat, and the cat sat\n",
            "the cat sat on the mat, and the cat sat on\n",
            "the cat sat on the mat, and the cat sat on the\n",
            "the cat sat on the mat, and the cat sat on the mat\n",
            "the cat sat on the mat, and the cat sat on the mat,\n",
            "the cat sat on the mat, and the cat sat on the mat, and\n",
            "the cat sat on the mat, and the cat sat on the mat, and the\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat on\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat on the\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat on the mat\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat on the mat,\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat on the mat, and\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat on the mat, and the\n",
            "the cat sat on the mat, and the cat sat on the mat, and the cat sat on the mat, and the cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full loop (generate multiple tokens)\n",
        "\n",
        "def generate_step_by_step(prompt, steps=10):\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tokens)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            next_token = sample_next_token(logits, temperature=0.8, top_k=40)\n",
        "\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "        print(tokenizer.decode(tokens[0]))\n",
        "\n",
        "generate_step_by_step(text, steps=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef1b6bf-57ff-4871-dfcf-bc5c8ed9fe0d",
        "id": "YdDrDL01KDHH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cat sat on the mat and\n",
            "the cat sat on the mat and let\n",
            "the cat sat on the mat and let her\n",
            "the cat sat on the mat and let her down\n",
            "the cat sat on the mat and let her down,\n",
            "the cat sat on the mat and let her down, so\n",
            "the cat sat on the mat and let her down, so that\n",
            "the cat sat on the mat and let her down, so that the\n",
            "the cat sat on the mat and let her down, so that the dog\n",
            "the cat sat on the mat and let her down, so that the dog could\n",
            "the cat sat on the mat and let her down, so that the dog could do\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else.\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else. He\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else. He did\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else. He did nothing\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else. He did nothing else\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else. He did nothing else except\n",
            "the cat sat on the mat and let her down, so that the dog could do nothing else. He did nothing else except watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full loop (generate multiple tokens)\n",
        "\n",
        "def generate_step_by_step(prompt, steps=10):\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tokens)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            next_token = sample_next_token(logits, temperature=2.0, top_k=40)\n",
        "\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "        print(tokenizer.decode(tokens[0]))\n",
        "\n",
        "generate_step_by_step(text, steps=20)\n"
      ],
      "metadata": {
        "id": "7Y-vgMelWkKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29290d0-a48a-476e-fdf2-9af6a632f8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cat sat on the mat,\n",
            "the cat sat on the mat, its\n",
            "the cat sat on the mat, its legs\n",
            "the cat sat on the mat, its legs wrapped\n",
            "the cat sat on the mat, its legs wrapped in\n",
            "the cat sat on the mat, its legs wrapped in soft\n",
            "the cat sat on the mat, its legs wrapped in soft leather\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets).\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part,\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part, to\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part, to do\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part, to do so\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part, to do so is\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part, to do so is the\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part, to do so is the little\n",
            "the cat sat on the mat, its legs wrapped in soft leather sheets). My favourite part, to do so is the little dog\n"
          ]
        }
      ]
    }
  ]
}
